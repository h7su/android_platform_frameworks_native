{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "2182eaaa_c0848f4e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1944400
      },
      "writtenOn": "2023-05-15T17:33:51Z",
      "side": 1,
      "message": "Brain dump of thoughts from trying to understand this CL, probably much of it wrong:\n\nSo, on the client side of a session with no incoming thread pool, there is no extra thread to process incoming commands, we can only do things opportunistically when the library user makes a transaction. Before, for oneway calls, we\u0027d only check for incoming commands if the `sendmsg` syscall blocked. Now, we\u0027ll always check after the `sendmsg` completes.\n\n--\n\nIt might be interesting to turn this into a general \"runtime\" hook that we can expand to handle more types of bookkeeping later (like draining the other connections as mentioned already). If we\u0027ve got an epoll context setup in advance for all the connections, it might be fast enough to always poll them all (and then ignore the results for connections that another thread is already holding). Could probably do clever things with uring if it was available.\n\n--\n\nFor use cases that send oneway calls at a high rate, it might be better to only process incoming commands occasionally. Maybe we could track the last time and only force it every N milliseconds. Best to start simple though.\n\n--\n\nThis is suppose to help with test flakes, but I\u0027d expect that it couldn\u0027t completely eliminate them since there is no coordination with the other side. Also, for prod, there is still a \"leaky\" scenario\n\n  1. client sends binder over oneway call\n  2. client checks drains commands\n  3. server finishes process the oneway call and sends decref command back\n  4. client doesn\u0027t send any more commands for a long time, so never sees the decrefs and we keep the binder object around wasting memory\n\nNot sure we can do anything about that besides requiring a non-empty threadpool.\n\nIn the spirit of making things more deterministic, I almost want to suggest we should do the drain *before* sending the oneway call.",
      "revId": "d0b54ce9ca2f6da3003ba353160e4dc700fa151c",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "606af688_37de70a2",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1120458
      },
      "writtenOn": "2023-05-15T22:38:31Z",
      "side": 1,
      "message": "re epoll - I\u0027d like to get real integration w/ epoll. We\u0027ve tried a hook w/ HIDL addPostCommandTask, but I\u0027m not sure it\u0027s worth it.\n\n\u003e So, on the client side of a session with no incoming thread pool, there is no extra thread to process incoming commands\n\nYeah, this is my hesitation now. I keep on thinking this is the issue David was hitting, but we are using incoming threadpools in the AVF cases.\n\n\u003e Not sure we can do anything about that besides requiring a non-empty threadpool.\n\nI\u0027m also leaning this way, but I don\u0027t think it\u0027ll be an option in all cases.\n\n\u003e In the spirit of making things more deterministic, I almost want to suggest we should do the drain before sending the oneway call.\n\nWhy would this make it more deterministic?",
      "parentUuid": "2182eaaa_c0848f4e",
      "revId": "d0b54ce9ca2f6da3003ba353160e4dc700fa151c",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "37b603ad_5f5cc56d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1944400
      },
      "writtenOn": "2023-05-15T22:57:54Z",
      "side": 1,
      "message": "\u003e Why would this make it more deterministic?\n\nIt means there is no chance a oneway call can process decrefs that happen in response to the request being sent. If you process them afterwards, then you may or may not get lucky with scheduling.",
      "parentUuid": "606af688_37de70a2",
      "revId": "d0b54ce9ca2f6da3003ba353160e4dc700fa151c",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ee8ee7c4_6d7bfd31",
        "filename": "libs/binder/RpcState.cpp",
        "patchSetId": 1
      },
      "lineNbr": 607,
      "author": {
        "id": 1809582
      },
      "writtenOn": "2023-05-12T00:25:32Z",
      "side": 1,
      "message": "Is there any guarantee that the DEC_STRONG messages came back on the same connection?\n\nIt might be useful to do this for all available connections in that session, but that might be too much overhead. Wdyt?",
      "revId": "d0b54ce9ca2f6da3003ba353160e4dc700fa151c",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ec581862_6c1cc203",
        "filename": "libs/binder/RpcState.cpp",
        "patchSetId": 1
      },
      "lineNbr": 607,
      "author": {
        "id": 1120458
      },
      "writtenOn": "2023-05-12T23:25:05Z",
      "side": 1,
      "message": "That\u0027s kind of what I was suggesting/thinking in the commit message, but it needs to happen for non-oneway calls as well, and it could cause a lot of lock contention.\n\nFor sessions with incoming threadpools though, we should never send control commands on these connections in-between calls. However, we don\u0027t have a test for this right now, and I need to add one.",
      "parentUuid": "ee8ee7c4_6d7bfd31",
      "revId": "d0b54ce9ca2f6da3003ba353160e4dc700fa151c",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "876de13a_faa4eee7",
        "filename": "libs/binder/RpcState.cpp",
        "patchSetId": 1
      },
      "lineNbr": 607,
      "author": {
        "id": 1809582
      },
      "writtenOn": "2023-05-12T23:39:36Z",
      "side": 1,
      "message": "Acknowledged",
      "parentUuid": "ec581862_6c1cc203",
      "revId": "d0b54ce9ca2f6da3003ba353160e4dc700fa151c",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    }
  ]
}