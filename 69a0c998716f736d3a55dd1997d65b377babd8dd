{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "49034ae6_49d734ba",
        "filename": "libs/binder/RpcTransportRaw.cpp",
        "patchSetId": 41
      },
      "lineNbr": 223,
      "author": {
        "id": 1944400
      },
      "writtenOn": "2022-06-29T05:46:31Z",
      "side": 1,
      "message": "One of the test failures was caused by checking the CMSG_DATA even when an error occurred.\n\nIn the error case, `CMSG_FIRSTHDR` returns null for glibc and non-null for bionic. The bionic behavior causes the code to go into an infinite loop.\n\nThere is a comment the glibc code about this\n\n```\n  if ((size_t) __cmsg-\u003ecmsg_len \u003c sizeof (struct cmsghdr))\n    /* The kernel header does this so there may be a reason.  */\n    return (struct cmsghdr *) 0;\n```\n\nI wonder if I just found the reason :) maybe will send a bug to the bionic team",
      "range": {
        "startLine": 221,
        "startChar": 0,
        "endLine": 223,
        "endChar": 17
      },
      "revId": "69a0c998716f736d3a55dd1997d65b377babd8dd",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "98d94c64_56c2a954",
        "filename": "libs/binder/RpcTransportRaw.cpp",
        "patchSetId": 41
      },
      "lineNbr": 266,
      "author": {
        "id": 1120458
      },
      "writtenOn": "2022-06-29T23:40:11Z",
      "side": 1,
      "message": "is there any reason to have this separate, VS as part of interruptableReadFully? Right now, I wonder what the risk is, if FDs are attached as part of one read, then they aren\u0027t consumed (because consumePendingAncillaryData is called), and then at the next read, these FDs are consumed. Someone could pass FDs in one write, and they would get consumed during another?\n\nOtherwise, do we need the append behavior? (*fds \u003d std::move(mFdsPendingRead);)?",
      "range": {
        "startLine": 258,
        "startChar": 0,
        "endLine": 266,
        "endChar": 5
      },
      "revId": "69a0c998716f736d3a55dd1997d65b377babd8dd",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b0f8e2b6_d1bd1133",
        "filename": "libs/binder/RpcTransportRaw.cpp",
        "patchSetId": 41
      },
      "lineNbr": 266,
      "author": {
        "id": 1944400
      },
      "writtenOn": "2022-06-30T00:07:49Z",
      "side": 1,
      "message": "Discussed over chat some. I\u0027m going to try it as a quick follow up\n\nRe append: Not necessary but I think this is better because we aren\u0027t forced to re-allocate a buffer each time (the caller is currently reallocing each time, but that isn\u0027t necessary in this design)",
      "parentUuid": "98d94c64_56c2a954",
      "range": {
        "startLine": 258,
        "startChar": 0,
        "endLine": 266,
        "endChar": 5
      },
      "revId": "69a0c998716f736d3a55dd1997d65b377babd8dd",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "cde75f10_b9a9dc52",
        "filename": "libs/binder/include/binder/RpcServer.h",
        "patchSetId": 41
      },
      "lineNbr": 207,
      "author": {
        "id": 1120458
      },
      "writtenOn": "2022-06-29T23:40:11Z",
      "side": 1,
      "message": "nit: bitset::set returns *this so could call this in the same way it is set in RpcServer.cpp.",
      "range": {
        "startLine": 207,
        "startChar": 0,
        "endLine": 207,
        "endChar": 93
      },
      "revId": "69a0c998716f736d3a55dd1997d65b377babd8dd",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    }
  ]
}